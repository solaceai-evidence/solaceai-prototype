version: "3.8"

services:
  # Load balancer
  nginx-lb:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./proxy/load-balancer.conf:/etc/nginx/nginx.conf
    depends_on:
      - api-1
      - api-2
      - api-3

  # Multiple API instances for parallel processing
  api-1:
    build: ./api
    environment:
      - CONFIG_PATH=run_configs/default.json
      - WORKER_ID=1
    volumes:
      - ./api:/app
      - ./api/logs:/app/logs
    expose:
      - "8000"

  api-2:
    build: ./api
    environment:
      - CONFIG_PATH=run_configs/default.json
      - WORKER_ID=2
    volumes:
      - ./api:/app
      - ./api/logs:/app/logs
    expose:
      - "8000"

  api-3:
    build: ./api
    environment:
      - CONFIG_PATH=run_configs/default.json
      - WORKER_ID=3
    volumes:
      - ./api:/app
      - ./api/logs:/app/logs
    expose:
      - "8000"

  # Dedicated ONNX reranker service (high-performance)
  reranker:
    build:
      context: ./reranker_service
      dockerfile: Dockerfile.onnx
    container_name: onnx-reranker-service-scaled
    environment:
      # Model Configuration
      - MODEL_NAME_OR_PATH=mixedbread-ai/mxbai-rerank-large-v1
      - ONNX_PATH=/app/reranker_model.onnx
      - TOKENIZER_PATH=/app/tokenizer/
      - MAX_LENGTH=512
      - BATCH_SIZE=64 # Larger batch size for scaled deployment

      # Service Configuration
      - AUTO_CONVERT=true
      - PORT=8001
      - HOST=0.0.0.0
      - SERVICE_NAME=onnx-reranker-service

      # GPU Configuration
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # Performance Configuration
      - LOG_LEVEL=INFO
      - PYTHONPATH=/app
    ports:
      - "8001:8001"
    volumes:
      # Mount ScholarQA config for automatic integration
      - ./api/run_configs/default.json:/app/config.json:ro
      # Persist model files to avoid re-downloading/converting
      - onnx_models_scaled:/app/models
      - onnx_tokenizers_scaled:/app/tokenizer
      # Cache for Hugging Face models
      - huggingface_cache_scaled:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 300s # Allow time for model conversion

  ui:
    build: ./ui
    ports:
      - "3000:3000"
    volumes:
      - ./ui:/app
      - /app/node_modules

  sonar:
    build: ./sonar
    ports:
      - "8888:8888"

volumes:
  onnx_models_scaled:
    driver: local
  onnx_tokenizers_scaled:
    driver: local
  huggingface_cache_scaled:
    driver: local
